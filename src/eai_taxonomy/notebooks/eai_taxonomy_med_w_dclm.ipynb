{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EAI Taxonomy STEM w/ DCLM\n",
    "\n",
    "This notebook demonstrates how to curate the EAI Taxonomy STEM dataset with DCLM filtering, a 912B token collection of high-quality STEM documents filtered from the Essential AI Common Crawl using semantic taxonomy labels and instruction density classification.\n",
    " \n",
    "## Overview\n",
    "The EAI Taxonomy STEM dataset represents a novel approach to STEM dataset curation. Rather than relying solely on domain-specific classifiers, we leverage semantic taxonomy labels to identify documents that:\n",
    "\n",
    " - Contain science, engineering, medical, and computer science content\n",
    " - Demonstrate reasoning capabilities \n",
    " - Maintain high technical correctness\n",
    " - Come from high-quality document types per sub-topic\n",
    "\n",
    "Key Statistics:\n",
    " - Base STEM Size: 1.74T tokens\n",
    " - STEM w/ DCLM Size: 912B tokens\n",
    " - Performance: 34.5% on MMLU-STEM (+6.8pp vs DCLM-baseline)\n",
    " - Hybrid curation: Combines taxonomy filters with DCLM instruction density classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Initialize Spark Session and Load Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import json\n",
    "\n",
    "# Set up visualization style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "class SessionFactory:\n",
    "    @staticmethod\n",
    "    def create(\n",
    "        autoscale=False,\n",
    "        max_partition_bytes=int(2**28),  # 256MB\n",
    "        name=None,\n",
    "        num_cores=None,\n",
    "        num_instances=None,\n",
    "        use_arrow=False,\n",
    "        speculative_execution=True,\n",
    "        additional_conf={},\n",
    "    ) -> tuple[SparkSession, logging.Logger]:\n",
    "        # Set Spark configurations\n",
    "        conf = SparkConf()\n",
    "        conf.set(\"spark.task.maxFailures\", \"15\")\n",
    "        conf.set(\"spark.sql.sources.parallelPartitionDiscovery.parallelism\", \"250\")\n",
    "        conf.set(\"spark.sql.files.maxPartitionBytes\", str(max_partition_bytes))\n",
    "        conf.set(\n",
    "            \"spark.sql.hive.filesourcePartitionFileCacheSize\", 16 * 1024 * 1024 * 1024\n",
    "        )\n",
    "\n",
    "        if name:\n",
    "            conf.set(\"spark.app.name\", name)\n",
    "\n",
    "        if not autoscale:\n",
    "            assert num_instances is not None and num_cores is not None, (\n",
    "                \"num_instances and num_cores must be set if autoscale is False\"\n",
    "            )\n",
    "            conf.set(\"spark.dynamicAllocation.enabled\", \"false\")\n",
    "            conf.set(\"spark.executor.instances\", str(num_instances))\n",
    "            conf.set(\"spark.executor.cores\", str(num_cores))\n",
    "            conf.set(\"spark.sql.shuffle.partitions\", str(num_cores * num_instances))\n",
    "            conf.set(\"spark.default.parallelism\", str(num_cores * num_instances))\n",
    "\n",
    "        if use_arrow:\n",
    "            conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", True)\n",
    "            conf.set(\"spark.sql.execution.arrow.maxRecordsPerBatch\", 10000)\n",
    "\n",
    "        for k, v in additional_conf.items():\n",
    "            conf.set(k, v)\n",
    "\n",
    "        if speculative_execution:\n",
    "            # Add configuration for handling stragglers\n",
    "            conf.set(\"spark.speculation\", \"true\")\n",
    "            conf.set(\n",
    "                \"spark.speculation.interval\", \"5000ms\"\n",
    "            )  # Check for stragglers every 5 seconds\n",
    "            conf.set(\n",
    "                \"spark.speculation.multiplier\", \"3\"\n",
    "            )  # Task is a straggler if it's running 3x longer than median\n",
    "            conf.set(\"spark.speculation.quantile\", \"0.75\")\n",
    "\n",
    "        # Start the SparkSession\n",
    "        builder: SparkSession.Builder = SparkSession.builder\n",
    "        spark = builder.config(conf=conf).getOrCreate()\n",
    "        spark.sql(\"set spark.sql.files.ignoreCorruptFiles=true\")\n",
    "        spark.sql(\"set spark.sql.files.ignoreMissingFiles=true\")\n",
    "\n",
    "        logging.getLogger(\"py4j\").setLevel(logging.WARNING)\n",
    "        logging.getLogger(\"pyspark\").setLevel(logging.WARNING)\n",
    "\n",
    "        # Retrieve Spark's Log4j logger via the Py4J JVM bridge so callers can\n",
    "        # emit messages that appear in the same place as the rest of the Spark\n",
    "        # runtime logs. If an application name was supplied, we use that as the\n",
    "        # logger name; otherwise we fall back to this module's __name__.\n",
    "        log4j = spark._jvm.org.apache.log4j\n",
    "        spark_logger = log4j.LogManager.getLogger(name if name else __name__)\n",
    "\n",
    "        # Return both the SparkSession and the Java Log4j logger so downstream\n",
    "        # code can keep a reference to the shared logger instance.\n",
    "        return spark, spark_logger\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from eai_taxonomy.infra.spark_session_factory import SessionFactory\n",
    "\n",
    "DATA_PATH = \"<INPUT_PATH>\"\n",
    "NUM_INSTANCES = 50\n",
    "NUM_CORES = 110\n",
    "\n",
    "spark, logger = SessionFactory.create(name=\"eai-taxonomy-stem-w-dclm\", num_instances=NUM_INSTANCES, num_cores=NUM_CORES)\n",
    "df = spark.read.parquet(DATA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define the Taxonomy Top Math Filter\n",
    "The filter combines multiple taxonomy dimensions to identify high-quality mathematical content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "science_codes = [\"50\", \"51\", \"54\", \"57\", \"58\", \"59\", \"61\"]\n",
    "\n",
    "dds_primary = col(\"eai_taxonomy.free_decimal_correspondence.primary.code\").substr(0, 2)\n",
    "dds_secondary = col(\"eai_taxonomy.free_decimal_correspondence.secondary.code\").substr(0, 2)\n",
    "\n",
    "dds_filter = (\n",
    "    (\n",
    "        (dds_primary == \"61\") & (dds_secondary.isin(science_codes))\n",
    "    ) |\n",
    "    (\n",
    "        (dds_secondary == \"61\") & (dds_primary.isin(science_codes))\n",
    "    )\n",
    ")\n",
    "\n",
    "# Add document type constraints:\n",
    "doc_type_v1_codes = [\"2\", \"3\"]  # Academic/Research, Reference/Encyclopedic/Educational\n",
    "doc_type_v2_codes = [\"3\", \"8\", \"10\", \"18\"]  # Academic Writing, Documentation, Knowledge Article, Q&A Forum\n",
    "\n",
    "# Document type blacklists\n",
    "DOC_TYPE_V1_BLACKLIST = [\"1\", \"4\", \"5\", \"6\", \"8\", \"9\", \"10\", \"15\", \"16\"]\n",
    "DOC_TYPE_V2_BLACKLIST = [\"1\", \"2\", \"4\", \"5\", \"6\", \"7\", \"11\", \"12\", \"13\", \"14\", \"16\", \"17\", \"19\", \"20\", \"22\", \"23\", \"24\"]\n",
    "\n",
    "doc_type_v1_col = col(\"eai_taxonomy.document_type_v1.primary.code\")\n",
    "doc_type_v2_col = col(\"eai_taxonomy.document_type_v2.primary.code\")\n",
    "\n",
    "doc_type_filter = (\n",
    "    doc_type_v1_col.isin(doc_type_v1_codes) |\n",
    "    doc_type_v2_col.isin(doc_type_v2_codes)\n",
    ") & (\n",
    "    ~doc_type_v1_col.isin(DOC_TYPE_V1_BLACKLIST) &\n",
    "    ~doc_type_v2_col.isin(DOC_TYPE_V2_BLACKLIST)\n",
    ")\n",
    "        \n",
    "# Combine DDS and document type filters\n",
    "final_filter = dds_filter & doc_type_filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Apply the Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stem_df = df.filter(stem_filter)\n",
    "\n",
    "total_docs = df.count()\n",
    "stem_docs = stem_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Save the Filtered Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = \"<OUTPUT_PATH>\"\n",
    "stem_df.write.parquet(OUTPUT_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
