{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EAI Taxonomy Top Math\n",
    "\n",
    "This notebook demonstrates how to curate the Taxonomy Top Math dataset, a 29B token collection of high-quality mathematical documents filtered from the Essential AI Common Crawl using semantic taxonomy labels.\n",
    "\n",
    "## Overview\n",
    "The Taxonomy Top Math dataset represents a novel approach to mathematical dataset curation. Rather than long iteration cycles with domain-specific classifiers, we leverage semantic taxonomy labels to identify documents that:\n",
    "\n",
    " - Contain mathematical content (FDC code 51)\n",
    " - Demonstrate reasoning capabilities\n",
    " - Maintain high technical correctness\n",
    " - Come from educational or reference sources\n",
    "\n",
    "Key Statistics:\n",
    " - Size: 29B tokens\n",
    " - Documents: 19.8M documents (from 23.6B in Essential Common Crawl)\n",
    " - Performance: 21.3% on GSM8K, 11.0% on MATH\n",
    " - No domain-specific curation: Uses only taxonomy filters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Initialize Spark Session and Load Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import json\n",
    "\n",
    "# Set up visualization style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "class SessionFactory:\n",
    "    @staticmethod\n",
    "    def create(\n",
    "        autoscale=False,\n",
    "        max_partition_bytes=int(2**28),  # 256MB\n",
    "        name=None,\n",
    "        num_cores=None,\n",
    "        num_instances=None,\n",
    "        use_arrow=False,\n",
    "        speculative_execution=True,\n",
    "        additional_conf={},\n",
    "    ) -> tuple[SparkSession, logging.Logger]:\n",
    "        # Set Spark configurations\n",
    "        conf = SparkConf()\n",
    "        conf.set(\"spark.task.maxFailures\", \"15\")\n",
    "        conf.set(\"spark.sql.sources.parallelPartitionDiscovery.parallelism\", \"250\")\n",
    "        conf.set(\"spark.sql.files.maxPartitionBytes\", str(max_partition_bytes))\n",
    "        conf.set(\n",
    "            \"spark.sql.hive.filesourcePartitionFileCacheSize\", 16 * 1024 * 1024 * 1024\n",
    "        )\n",
    "\n",
    "        if name:\n",
    "            conf.set(\"spark.app.name\", name)\n",
    "\n",
    "        if not autoscale:\n",
    "            assert num_instances is not None and num_cores is not None, (\n",
    "                \"num_instances and num_cores must be set if autoscale is False\"\n",
    "            )\n",
    "            conf.set(\"spark.dynamicAllocation.enabled\", \"false\")\n",
    "            conf.set(\"spark.executor.instances\", str(num_instances))\n",
    "            conf.set(\"spark.executor.cores\", str(num_cores))\n",
    "            conf.set(\"spark.sql.shuffle.partitions\", str(num_cores * num_instances))\n",
    "            conf.set(\"spark.default.parallelism\", str(num_cores * num_instances))\n",
    "\n",
    "        if use_arrow:\n",
    "            conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", True)\n",
    "            conf.set(\"spark.sql.execution.arrow.maxRecordsPerBatch\", 10000)\n",
    "\n",
    "        for k, v in additional_conf.items():\n",
    "            conf.set(k, v)\n",
    "\n",
    "        if speculative_execution:\n",
    "            # Add configuration for handling stragglers\n",
    "            conf.set(\"spark.speculation\", \"true\")\n",
    "            conf.set(\n",
    "                \"spark.speculation.interval\", \"5000ms\"\n",
    "            )  # Check for stragglers every 5 seconds\n",
    "            conf.set(\n",
    "                \"spark.speculation.multiplier\", \"3\"\n",
    "            )  # Task is a straggler if it's running 3x longer than median\n",
    "            conf.set(\"spark.speculation.quantile\", \"0.75\")\n",
    "\n",
    "        # Start the SparkSession\n",
    "        builder: SparkSession.Builder = SparkSession.builder\n",
    "        spark = builder.config(conf=conf).getOrCreate()\n",
    "        spark.sql(\"set spark.sql.files.ignoreCorruptFiles=true\")\n",
    "        spark.sql(\"set spark.sql.files.ignoreMissingFiles=true\")\n",
    "\n",
    "        logging.getLogger(\"py4j\").setLevel(logging.WARNING)\n",
    "        logging.getLogger(\"pyspark\").setLevel(logging.WARNING)\n",
    "\n",
    "        # Retrieve Spark's Log4j logger via the Py4J JVM bridge so callers can\n",
    "        # emit messages that appear in the same place as the rest of the Spark\n",
    "        # runtime logs. If an application name was supplied, we use that as the\n",
    "        # logger name; otherwise we fall back to this module's __name__.\n",
    "        log4j = spark._jvm.org.apache.log4j\n",
    "        spark_logger = log4j.LogManager.getLogger(name if name else __name__)\n",
    "\n",
    "        # Return both the SparkSession and the Java Log4j logger so downstream\n",
    "        # code can keep a reference to the shared logger instance.\n",
    "        return spark, spark_logger\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from eai_taxonomy.infra.spark_session_factory import SessionFactory\n",
    "\n",
    "DATA_PATH = \"gs://consus-dataproc/taxonomy/hf/raw/*/*/*.parquet\"\n",
    "NUM_INSTANCES = 50\n",
    "NUM_CORES = 110\n",
    "\n",
    "spark, logger = SessionFactory.create(name=\"eai-taxonomy-top-math\", num_instances=NUM_INSTANCES, num_cores=NUM_CORES)\n",
    "df = spark.read.parquet(DATA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define the Taxonomy Top Math Filter\n",
    "The filter combines multiple taxonomy dimensions to identify high-quality mathematical content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Define filter components based on the paper\n",
    "DOC_TYPE_V1 = [\n",
    "    \"Reference/Encyclopedic/Educational\",\n",
    "    \"Code/Software\",\n",
    "    \"Social/Forum\",\n",
    "    \"Personal/Misc\",\n",
    "]\n",
    "\n",
    "DOC_TYPE_V2 = [\n",
    "    \"Comment Section\",\n",
    "    \"Documentation\",\n",
    "    \"FAQ\",\n",
    "    \"Knowledge Article\",\n",
    "    \"Nonfiction Writing\",\n",
    "    \"Personal Blog\",\n",
    "    \"Q&A Forum\",\n",
    "    \"Structured Data\",\n",
    "    \"Tutorial\",\n",
    "]\n",
    "\n",
    "REASONING_DEPTH = [\n",
    "    \"Basic Reasoning\",\n",
    "    \"Intermediate Reasoning\",\n",
    "    \"Advanced Reasoning\",\n",
    "    \"Exceptional Reasoning\",\n",
    "]\n",
    "\n",
    "TECH_CORRECTNESS = [\"Highly Correct\", \"Exceptionally Correct\"]\n",
    "\n",
    "# Free Decimal Correspondence: 51 = Mathematics\n",
    "FDC_KEEP = [\"51\"]\n",
    "\n",
    "filter = (\n",
    "    col(\"eai_taxonomy.free_decimal_correspondence.primary.code\").startswith(\"51\")\n",
    "    & col(\"eai_taxonomy.document_type_v1.primary.label\").isin(DOC_TYPE_V1)\n",
    "    & col(\"eai_taxonomy.document_type_v2.primary.label\").isin(DOC_TYPE_V2)\n",
    "    & col(\"eai_taxonomy.reasoning_depth.primary.label\").isin(REASONING_DEPTH)\n",
    "    & col(\"eai_taxonomy.technical_correctness.primary.label\").isin(TECH_CORRECTNESS)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Apply the Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "math_df = df.filter(filter)\n",
    "math_df.cache()\n",
    "\n",
    "total_docs = df.count()\n",
    "math_docs = math_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Selection rate: {math_docs/total_docs:.4%}\")\n",
    "print(f\"Reduction: {(1 - math_docs/total_docs):.2%} filtered out\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Save the Filtered Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = \"<OUTPUT_PATH>\"\n",
    "math_df.write.parquet(OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Analyze Filter Components Distribution\n",
    "\n",
    "Let's examine how each filter component contributes to the final dataset:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FDC Mathematics subcategories (51X)\n",
    "math_subcategories = {\n",
    "    \"510\": \"Mathematics (General)\",\n",
    "    \"511\": \"General principles of mathematics\",\n",
    "    \"512\": \"Algebra\",\n",
    "    \"513\": \"Arithmetic\", \n",
    "    \"514\": \"Topology\",\n",
    "    \"515\": \"Analysis\",\n",
    "    \"516\": \"Geometry\",\n",
    "    \"517\": \"[Unassigned]\",\n",
    "    \"518\": \"Numerical analysis\",\n",
    "    \"519\": \"Probabilities & applied mathematics\"\n",
    "}\n",
    "\n",
    "# Get distribution of math subcategories in our filtered dataset\n",
    "subcategory_dist = math_df.groupBy(\n",
    "    F.substring(F.col(\"eai_taxonomy.free_decimal_correspondence.primary.code\"), 1, 3)\n",
    ").count().collect()\n",
    "\n",
    "# Convert to dictionary\n",
    "subcategory_counts = {row[0]: row[1] for row in subcategory_dist}\n",
    "\n",
    "# Create visualization\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Pie chart\n",
    "sizes = [subcategory_counts.get(cat, 0) for cat in math_subcategories.keys()]\n",
    "labels = [f\"{cat}: {math_subcategories[cat]}\" for cat in math_subcategories.keys() if subcategory_counts.get(cat, 0) > 0]\n",
    "sizes_filtered = [s for s in sizes if s > 0]\n",
    "\n",
    "ax1.pie(sizes_filtered, labels=labels, autopct='%1.1f%%', startangle=90)\n",
    "ax1.set_title('Distribution of Mathematics Subcategories in Dataset')\n",
    "\n",
    "# Bar chart with counts\n",
    "categories = [cat for cat, count in subcategory_counts.items() if count > 0]\n",
    "counts = [count for count in subcategory_counts.values() if count > 0]\n",
    "\n",
    "ax2.bar(range(len(categories)), counts)\n",
    "ax2.set_xticks(range(len(categories)))\n",
    "ax2.set_xticklabels(categories, rotation=45)\n",
    "ax2.set_ylabel('Number of Documents')\n",
    "ax2.set_title('Document Counts by Mathematics Subcategory')\n",
    "ax2.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'{x/1e6:.1f}M'))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print exact counts\n",
    "print(\"\\nMathematics subcategory distribution:\")\n",
    "for code, name in math_subcategories.items():\n",
    "    count = subcategory_counts.get(code, 0)\n",
    "    if count > 0:\n",
    "        print(f\"{code} - {name}: {count:,} documents ({count/math_docs:.1%})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
