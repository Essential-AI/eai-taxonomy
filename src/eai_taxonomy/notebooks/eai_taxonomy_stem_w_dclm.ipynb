{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EAI Taxonomy STEM w/ DCLM\n",
    "\n",
    "This notebook demonstrates how to curate the EAI Taxonomy STEM dataset with DCLM filtering, a 912B token collection of high-quality STEM documents filtered from the Essential AI Common Crawl using semantic taxonomy labels and instruction density classification.\n",
    " \n",
    "## Overview\n",
    "The EAI Taxonomy STEM dataset represents a novel approach to STEM dataset curation. Rather than relying solely on domain-specific classifiers, we leverage semantic taxonomy labels to identify documents that:\n",
    "\n",
    " - Contain science, engineering, medical, and computer science content\n",
    " - Demonstrate reasoning capabilities \n",
    " - Maintain high technical correctness\n",
    " - Come from high-quality document types per sub-topic\n",
    "\n",
    "Key Statistics:\n",
    " - Base STEM Size: 1.74T tokens\n",
    " - STEM w/ DCLM Size: 912B tokens\n",
    " - Performance: 34.5% on MMLU-STEM (+6.8pp vs DCLM-baseline)\n",
    " - Hybrid curation: Combines taxonomy filters with DCLM instruction density classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Initialize Spark Session and Load Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import json\n",
    "\n",
    "# Set up visualization style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "class SessionFactory:\n",
    "    @staticmethod\n",
    "    def create(\n",
    "        autoscale=False,\n",
    "        max_partition_bytes=int(2**28),  # 256MB\n",
    "        name=None,\n",
    "        num_cores=None,\n",
    "        num_instances=None,\n",
    "        use_arrow=False,\n",
    "        speculative_execution=True,\n",
    "        additional_conf={},\n",
    "    ) -> tuple[SparkSession, logging.Logger]:\n",
    "        # Set Spark configurations\n",
    "        conf = SparkConf()\n",
    "        conf.set(\"spark.task.maxFailures\", \"15\")\n",
    "        conf.set(\"spark.sql.sources.parallelPartitionDiscovery.parallelism\", \"250\")\n",
    "        conf.set(\"spark.sql.files.maxPartitionBytes\", str(max_partition_bytes))\n",
    "        conf.set(\n",
    "            \"spark.sql.hive.filesourcePartitionFileCacheSize\", 16 * 1024 * 1024 * 1024\n",
    "        )\n",
    "\n",
    "        if name:\n",
    "            conf.set(\"spark.app.name\", name)\n",
    "\n",
    "        if not autoscale:\n",
    "            assert num_instances is not None and num_cores is not None, (\n",
    "                \"num_instances and num_cores must be set if autoscale is False\"\n",
    "            )\n",
    "            conf.set(\"spark.dynamicAllocation.enabled\", \"false\")\n",
    "            conf.set(\"spark.executor.instances\", str(num_instances))\n",
    "            conf.set(\"spark.executor.cores\", str(num_cores))\n",
    "            conf.set(\"spark.sql.shuffle.partitions\", str(num_cores * num_instances))\n",
    "            conf.set(\"spark.default.parallelism\", str(num_cores * num_instances))\n",
    "\n",
    "        if use_arrow:\n",
    "            conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", True)\n",
    "            conf.set(\"spark.sql.execution.arrow.maxRecordsPerBatch\", 10000)\n",
    "\n",
    "        for k, v in additional_conf.items():\n",
    "            conf.set(k, v)\n",
    "\n",
    "        if speculative_execution:\n",
    "            # Add configuration for handling stragglers\n",
    "            conf.set(\"spark.speculation\", \"true\")\n",
    "            conf.set(\n",
    "                \"spark.speculation.interval\", \"5000ms\"\n",
    "            )  # Check for stragglers every 5 seconds\n",
    "            conf.set(\n",
    "                \"spark.speculation.multiplier\", \"3\"\n",
    "            )  # Task is a straggler if it's running 3x longer than median\n",
    "            conf.set(\"spark.speculation.quantile\", \"0.75\")\n",
    "\n",
    "        # Start the SparkSession\n",
    "        builder: SparkSession.Builder = SparkSession.builder\n",
    "        spark = builder.config(conf=conf).getOrCreate()\n",
    "        spark.sql(\"set spark.sql.files.ignoreCorruptFiles=true\")\n",
    "        spark.sql(\"set spark.sql.files.ignoreMissingFiles=true\")\n",
    "\n",
    "        logging.getLogger(\"py4j\").setLevel(logging.WARNING)\n",
    "        logging.getLogger(\"pyspark\").setLevel(logging.WARNING)\n",
    "\n",
    "        # Retrieve Spark's Log4j logger via the Py4J JVM bridge so callers can\n",
    "        # emit messages that appear in the same place as the rest of the Spark\n",
    "        # runtime logs. If an application name was supplied, we use that as the\n",
    "        # logger name; otherwise we fall back to this module's __name__.\n",
    "        log4j = spark._jvm.org.apache.log4j\n",
    "        spark_logger = log4j.LogManager.getLogger(name if name else __name__)\n",
    "\n",
    "        # Return both the SparkSession and the Java Log4j logger so downstream\n",
    "        # code can keep a reference to the shared logger instance.\n",
    "        return spark, spark_logger\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from eai_taxonomy.infra.spark_session_factory import SessionFactory\n",
    "\n",
    "DATA_PATH = \"<INPUT_PATH>\"\n",
    "NUM_INSTANCES = 50\n",
    "NUM_CORES = 110\n",
    "\n",
    "spark, logger = SessionFactory.create(name=\"eai-taxonomy-stem-w-dclm\", num_instances=NUM_INSTANCES, num_cores=NUM_CORES)\n",
    "df = spark.read.parquet(DATA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define the Taxonomy Top Math Filter\n",
    "The filter combines multiple taxonomy dimensions to identify high-quality mathematical content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Define document type lists for each domain with code meanings\n",
    "\n",
    "# Code domain document types\n",
    "code_document_type_v1 = [\n",
    "    \"2\",  # Academic/Research\n",
    "    \"3\",  # Reference/Encyclopedic/Educational\n",
    "    \"4\",  # Code/Software\n",
    "    \"5\",  # Social/Forum\n",
    "]\n",
    "code_document_type_v2 = [\n",
    "    \"3\",   # Academic Writing\n",
    "    \"5\",   # Comment Section\n",
    "    \"8\",   # Documentation\n",
    "    \"10\",  # Knowledge Article\n",
    "    \"16\",  # Personal Blog\n",
    "    \"18\",  # Q&A Forum\n",
    "    \"23\"   # Tutorial\n",
    "]\n",
    "\n",
    "# Medical domain document types\n",
    "medical_document_type_v1 = [\n",
    "    \"2\",   # Academic/Research\n",
    "    \"3\",   # Reference/Encyclopedic/Educational\n",
    "    \"4\",   # Code/Software\n",
    "    \"11\"   # Legal/Regulatory\n",
    "]\n",
    "medical_document_type_v2 = [\n",
    "    \"3\",   # Academic Writing\n",
    "    \"8\",   # Documentation\n",
    "    \"9\",   # FAQ\n",
    "    \"10\",  # Knowledge Article\n",
    "    \"14\",  # News Article\n",
    "    \"23\"   # Tutorial\n",
    "]\n",
    "\n",
    "# Engineering domain document types\n",
    "engineering_document_type_v1 = [\n",
    "    \"2\",   # Academic/Research\n",
    "    \"3\",   # Reference/Encyclopedic/Educational\n",
    "    \"9\",   # Personal/Misc\n",
    "    \"11\"   # Legal/Regulatory\n",
    "]\n",
    "engineering_document_type_v2 = [\n",
    "    \"3\",   # Academic Writing\n",
    "    \"4\",   # Audio Transcript\n",
    "    \"8\",   # Documentation\n",
    "    \"9\",   # FAQ\n",
    "    \"10\",  # Knowledge Article\n",
    "    \"14\",  # News Article\n",
    "    \"23\"   # Tutorial\n",
    "]\n",
    "\n",
    "# Default domain document types (sciences and other tech)\n",
    "default_document_type_v1 = [\n",
    "    \"2\",   # Academic/Research\n",
    "    \"3\"    # Reference/Encyclopedic/Educational\n",
    "]\n",
    "default_document_type_v2 = [\n",
    "    \"3\",   # Academic Writing\n",
    "    \"10\",  # Knowledge Article\n",
    "    \"14\"   # News Article\n",
    "]\n",
    "\n",
    "# Bad reasoning depths to filter out\n",
    "reasoning_depth_bad = [\"Abstain\", \"Indeterminate\"]\n",
    "\n",
    "# Define valid DDS categories\n",
    "science_categories = [\n",
    "    \"50\",  # Science (general)\n",
    "    \"51\",  # Mathematics\n",
    "    \"52\",  # Astronomy & allied sciences\n",
    "    \"53\",  # Physics\n",
    "    \"54\",  # Chemistry & allied sciences\n",
    "    \"55\",  # Earth sciences\n",
    "    \"56\",  # Paleontology\n",
    "    \"57\",  # Life sciences\n",
    "    \"58\",  # Plants (Botany)\n",
    "    \"59\"   # Animals (Zoology)\n",
    "]\n",
    "tech_categories = [\n",
    "    \"60\",  # Technology (general)\n",
    "    \"61\",  # Medicine & health\n",
    "    \"62\",  # Engineering & allied operations\n",
    "    \"66\",  # Chemical engineering\n",
    "    \"00\"   # Computer science & general works\n",
    "]\n",
    "valid_dds_prefixes = science_categories + tech_categories\n",
    "\n",
    "# Create the base DDS filter\n",
    "dds_filter = (\n",
    "    # Both primary and secondary DDS must start with valid categories\n",
    "    col(\"eai_taxonomy.free_decimal_correspondence.primary.code\").substr(0, 2).isin(valid_dds_prefixes) &\n",
    "    col(\"eai_taxonomy.free_decimal_correspondence.secondary.code\").substr(0, 2).isin(valid_dds_prefixes)\n",
    ")\n",
    "\n",
    "# Create document type filters for each domain\n",
    "code_filter = (\n",
    "    # DDS is programming-related (005.1: Computer programming, 005.4: Systems programming)\n",
    "    (col(\"eai_taxonomy.free_decimal_correspondence.primary.code\").substr(0, 5).isin([\"005.1\", \"005.4\"]) | \n",
    "     col(\"eai_taxonomy.free_decimal_correspondence.secondary.code\").substr(0, 5).isin([\"005.1\", \"005.4\"])) &\n",
    "    # Document types suitable for code content\n",
    "    (col(\"eai_taxonomy.document_type_v1.primary.code\").isin(code_document_type_v1)) &\n",
    "    (col(\"eai_taxonomy.document_type_v2.primary.code\").isin(code_document_type_v2))\n",
    ")\n",
    "\n",
    "medical_filter = (\n",
    "    # DDS is medical (61: Medicine & health)\n",
    "    ((col(\"eai_taxonomy.free_decimal_correspondence.primary.code\").substr(0, 2) == \"61\") |\n",
    "     (col(\"eai_taxonomy.free_decimal_correspondence.secondary.code\").substr(0, 2) == \"61\")) &\n",
    "    # Document types suitable for medical content\n",
    "    (col(\"eai_taxonomy.document_type_v1.primary.code\").isin(medical_document_type_v1)) &\n",
    "    (col(\"eai_taxonomy.document_type_v2.primary.code\").isin(medical_document_type_v2))\n",
    ")\n",
    "\n",
    "engineering_filter = (\n",
    "    # DDS is engineering (62: Engineering & allied operations)\n",
    "    ((col(\"eai_taxonomy.free_decimal_correspondence.primary.code\").substr(0, 2) == \"62\") |\n",
    "     (col(\"eai_taxonomy.free_decimal_correspondence.secondary.code\").substr(0, 2) == \"62\")) &\n",
    "    # Document types suitable for engineering content\n",
    "    (col(\"eai_taxonomy.document_type_v1.primary.code\").isin(engineering_document_type_v1)) &\n",
    "    (col(\"eai_taxonomy.document_type_v2.primary.code\").isin(engineering_document_type_v2))\n",
    ")\n",
    "\n",
    "# Default filter for sciences and other valid tech categories\n",
    "default_filter = (\n",
    "    # Not in code, medical, or engineering specific categories\n",
    "    ~(col(\"eai_taxonomy.free_decimal_correspondence.primary.code\").substr(0, 5).isin([\"005.1\", \"005.4\"]) | \n",
    "      col(\"eai_taxonomy.free_decimal_correspondence.secondary.code\").substr(0, 5).isin([\"005.1\", \"005.4\"])) &\n",
    "    ~(col(\"eai_taxonomy.free_decimal_correspondence.primary.code\").substr(0, 2) == \"61\") &\n",
    "    ~(col(\"eai_taxonomy.free_decimal_correspondence.secondary.code\").substr(0, 2) == \"62\") &\n",
    "    # But is in valid DDS categories (sciences 50-59 or tech 60, 66)\n",
    "    (col(\"eai_taxonomy.free_decimal_correspondence.primary.code\").substr(0, 2).isin(valid_dds_prefixes) |\n",
    "     col(\"eai_taxonomy.free_decimal_correspondence.secondary.code\").substr(0, 2).isin(valid_dds_prefixes)) &\n",
    "    # Apply default document type filters for general academic/reference content\n",
    "    (col(\"eai_taxonomy.document_type_v1.primary.code\").isin(default_document_type_v1)) &\n",
    "    (col(\"eai_taxonomy.document_type_v2.primary.code\").isin(default_document_type_v2))\n",
    ")\n",
    "\n",
    "# Reasoning depth filter (common to all)\n",
    "reasoning_filter = (\n",
    "    # Exclude uncertain reasoning depth classifications\n",
    "    ~(col(\"eai_taxonomy.reasoning_depth.primary.code\").isin(reasoning_depth_bad)) &\n",
    "    # Ensure reasoning depth is not null\n",
    "    (col(\"eai_taxonomy.reasoning_depth.primary.code\").isNotNull())\n",
    ")\n",
    "\n",
    "# Combine all filters\n",
    "stem_filter = (\n",
    "    # Must pass reasoning depth filter\n",
    "    reasoning_filter &\n",
    "    # Must be in valid DDS categories\n",
    "    dds_filter &\n",
    "    # Must match one of the domain-specific document type filters\n",
    "    (code_filter | medical_filter | engineering_filter | default_filter)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Apply the Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stem_df = df.filter(stem_filter)\n",
    "\n",
    "total_docs = df.count()\n",
    "stem_docs = stem_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Save the Filtered Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = \"<OUTPUT_PATH>\"\n",
    "stem_df.write.parquet(OUTPUT_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
